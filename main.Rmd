---
title: "Project M1"
author: "Andreas, Simon, Jess, Lars"
date: "14/9/2019"
output:
  html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---
# SparNord ATM transactions
In this project we would work around the following problem statement. To solve the statement we will use the ML-techniques which the course so far have presented for us.

>>Predict whether the transaction in the Spar Nord ATMs is a euro transaction or not. 

In order to answer our problem statement, weâ€™ve obtained a data set published by Spar Nord A/S. The data set contains data of all cash withdrawals made at their ATMs in Denmark in 2017. Based on the data set, we then want to set up a model which can 'predict' whether the transaction in the ATMs is in euros or not.

Source: https://sparnordopenbanking.com/OpenData.

The raw-dataset contains 2.5 million observations with 33 variables, which range from information about day, time and month to weather conditions, coordinates and cardtype. Some of the variables include NA, but those are primarily message codes and text related to the withdrawal (some kind of error). This mean the dataset are pretty complete and there are no need for data imputation or cleaning. The next step is to modify the data to our problem statement, which is descried in feature generation. Before we get to that section, we will present a visually overview of the raw-data with summary statistics and explanatory comments. 

```{r eval=FALSE, include=FALSE}
#pacman::p_load(tidyverse, ggforce, caret, VIM, gridExtra, 
#               ggmap, osmdata, sf, geosphere, yardstick, 
#               knitr, ggthemes, ggridges, kableExtra)
```


```{r PACKAGES, message=FALSE, warning=FALSE}
library(tidyverse)
library(ggforce)
library(caret)
library(yardstick)
library(knitr)
library(ggthemes)
library(ggridges)
library(kableExtra)
library(gridExtra)
library(VIM)
library(ggridges)
library(ggmap)
library(osmdata)
library(sf)
library(GGally)
library(FactoMineR)
library(geosphere)
library(sf)
library(Hmisc)

base <- "#1f83b4"

load(file="generated_data/sATM.Rdata")
load(file="generated_data/bATM.Rdata")

set.seed(20092019)
```


## Data
One of the most common errors in data science is not looking at the data, so we will start with this. The dataset includes 113 different ATM's from SparNord placed around Denmark. Each include coordinates, which will be usefull later for feature generation, but we can use these to plot the machines in a map.

```{r MAP, fig.height=6, fig.width=11, message=FALSE, warning=FALSE}
coord <- as.matrix(data.frame(min = c(8, 55), 
                              max = c(13, 58), 
                              row.names = c("x","y")))

s <- bATM %>% filter(atm_lat > coord[2,1] & atm_lat < coord[2,2] & 
                     atm_lon < coord[1,2] & atm_lon > coord[1,1]) %>% 
  group_by(atm_id) %>% 
  summarize(n   = n(),
            lat = first(atm_lat),
            lon = first(atm_lon),
            man = first(atm_manufacturer))


# First map ---------------------------------------------------------------
map9  <- get_stamenmap(coord, zoom = 9,  maptype = "toner-lite", force = TRUE)

p1 <- ggmap(map9) +
  geom_point(data = s, aes(lon,lat, size=n/1000), alpha = 0.8) + 
  scale_size(range = c(0.5, 8)) +
  labs(title="SparNord ATM transactions in 2017", 
       subtitle = "size equals number of transactions", size="N in 1000s")

p2 <- ggmap(map9) +
  stat_density_2d(data = s, aes(lon,lat, fill = ..level..), geom = "polygon", alpha = .4) +
  #scale_fill_viridis_c(option = "magma") +
  scale_fill_gradient2(midpoint=0.5,low = "black", mid = base, high = "white") +
  labs(title="SparNord ATM transactions in 2017", 
       subtitle = "Density of ATM placement")

grid.arrange(p1,p2, nrow=1)
```

In this first visualisation we see that the activity is based heavily around Nordjylland, which is not surprising if you know the bank. We also see that there is a substantial difference in the number of transactions at each machine.

Well, lets see how the transactions are distributed in time:

```{r fig.height=3, fig.width=10}
s1 <- bATM %>% group_by(month) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(month,n))   + geom_col(fill=base, width=0.8)     + 
  labs(title = "By month") + 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))

s2 <- bATM %>% group_by(day) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(day,n)) + geom_col(fill=base, color=base)         + 
  labs(title = "By day")

s3 <- bATM %>% group_by(weekday) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(weekday,n)) + geom_col(fill=base, width=0.8) + 
  labs(title = "By weekday") + 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))

s4 <- bATM %>% group_by(hour) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(hour,n))    + geom_col(fill=base, color=base)       + 
  labs(title = "By hour") + 
  scale_x_continuous(breaks=c(0,3,6,9,12,15,18,21,24))

grid.arrange(s1,s3,s2,s4, nrow=1)
```


```{r fig.height=3, fig.width=10}
bATM$Euro <- as.factor(ifelse(bATM$currency=="DKK", FALSE, TRUE))
s5 <- bATM %>% filter(Euro == T) %>% group_by(month) %>% summarize(n=n()) %>% ggplot(aes(month,n)) + geom_col(fill=base)

grid.arrange(s5)

#fATM %>% 
#  group_by(atm_id) %>% 
#  summarize(n=n(), area = first(weather_city_name)) %>% 
#  ggplot(aes(reorder(atm_id,n), n, fill=as.factor(area))) + 
#  geom_col()
```








## Feature generation

```{r eval=FALSE, message=FALSE, warning=FALSE}
# Features for the data ---------------------------------------------------
#sATM <- bATM

# Holidays ----------------------------------------------------------------
# Ferie i Nordjylland (http://skoleferie-dk.dk/skoleferie-nordjylland/)

Vinter <- ifelse(( sATM$month == "February" & 11 <= sATM$day & sATM$day <= 26), TRUE, FALSE)
Paaske <- ifelse(( sATM$month == "April"    & 8  <= sATM$day & sATM$day <= 17), TRUE, FALSE)
Efter  <- ifelse(( sATM$month == "October"  & 14 <= sATM$day & sATM$day <= 22), TRUE, FALSE)
Jul    <- ifelse(((sATM$month == "December" & 20 <= sATM$day) | (sATM$month=="January" & sATM$day<=4)), TRUE, FALSE)
Sommer <- ifelse(((sATM$month == "June"     & 23 <= sATM$day) | (sATM$month=="July"    & sATM$day>=1)| (sATM$month=="August" & sATM$day<=13)), TRUE, FALSE)
sATM$Holliday <- Jul | Vinter | Paaske | Sommer | Efter

# Distance to other ATMs --------------------------------------------------

mat <- as.matrix(data.frame(lon = sATM$atm_lon, 
                            lat = sATM$atm_lat))

Aal.luft <- c(9.842829962, 57.088999644)
Bil.luft <- c(9.150999396, 55.73749705)
Cph.luft <- c(12.650462,   55.620750)

#Distance is in meter and a straight line:
sATM$Aal.luft <- distHaversine(mat, Aal.luft, r = 6378137)
sATM$Bil.luft <- distHaversine(mat, Bil.luft, r = 6378137)
sATM$Cph.luft <- distHaversine(mat, Cph.luft, r = 6378137)

#Nearest airport in km:
sATM <- sATM %>% 
  mutate(Airportdist = round(pmin(sATM$Aal.luft,sATM$Bil.luft,sATM$Cph.luft)/1000,2))

#Divid clock
sATM$Time <- ifelse((sATM$hour >= 4  & sATM$hour <= 9),  "Morning", NA)
sATM$Time <- ifelse((sATM$hour >= 10 & sATM$hour <= 15), "Midday",  sATM$Time)
sATM$Time <- ifelse((sATM$hour >= 16 & sATM$hour <= 21), "Evening", sATM$Time)
sATM$Time <- ifelse((sATM$hour >= 22 | sATM$hour <= 3),  "Night",   sATM$Time)

#Divid the days
sATM$Weekend <- ifelse(sATM$weekday == "Saturday" | sATM$weekday == "Sunday", TRUE, FALSE)
#Valuta
sATM$Euro <- as.factor(ifelse(sATM$currency=="DKK", FALSE, TRUE))
#Payout
sATM$Payout <- ifelse((sATM$day <=4 | sATM$day >= 27), TRUE, FALSE)
#Customer
sATM$Customer <- str_detect(sATM$card_type, pattern = "on-us")

#Cardtype
sATM$Card <- sapply(strsplit(sATM$card_type, split = ' - on-us', fixed = TRUE), function(x) (x[1]))
sATM$Card <- str_to_lower(sATM$Card)
sATM$Card <- capitalize(sATM$Card)
sATM$Card <- as.factor(sATM$Card)

#Region
loc <- sATM %>% group_by(atm_id) %>% 
  summarize(x=first(atm_lon), y=first(atm_lat))

reg <- NA
for (i in 1:nrow(loc)) {
  aa <- read_csv(paste0("http://dawa.aws.dk/kommuner/reverse?x=",loc$x[i] ,"&y=",loc$y[i]))
  reg[i] <- as.character(aa[10,])
}
loc$Region <- reg

sATM <- merge(sATM,loc[,c(1,4)],by ="atm_id")
sATM$Region <- sapply(strsplit(sATM$Region, split='": "', fixed=TRUE), function(x) (x[2]))
sATM$Region <- as.factor(sATM$Region)

sATM$Id       <- as.factor(sATM$atm_id)
sATM$Time     <- as.factor(sATM$Time)
sATM$Holliday <- as.factor(sATM$Holliday)
sATM$Customer <- as.factor(sATM$Customer)
sATM$Weekend  <- as.factor(sATM$Weekend)
sATM$Payout   <- as.factor(sATM$Payout)

#Final dataset
fsATM <- sATM %>% select(Id, Holliday, Airportdist, Time, 
                         Euro, Customer, Card, Weekend, Payout, Region)

save(fsATM, file = "generated_data/fsATM.Rdata")
```



The first step is to prepare the raw data for our analysis. The raw dataset doesn't need to be cleaned, because it's pretty complete for our purpose. Based on the problem statement we want the algorithm to predict Euro or not Euro transaction from the following parameters:

- Holliday
  This parameter contains information about the specific day, is it a "normal" day or a day in the vacation, the parameter are therefore logical.     It's contructed on day and month from the raw dataset. Information about vacation are taken from: http://skoleferie-dk.dk/skoleferie-nordjylland/.   We argue that the purpose for Euro are bigger in the vacation then else, which should provide usefull information to the algorithm.
  
- Airport distance
  This parameter contains information about the distance to the closest airport, the parameter are therefore numerical. It's contructed on the        longitude and laditude on each ATM from the raw dataset and the included airports. The included airports are Aalborg, Billund and Copenhagen. The   hypotese is that, when flying your destination are likely an Euro-country, which means that ATM closest to an airport would be used to get euro.
  
- Time
  This parameter contains information about the time of the day, which are grouped into 4: morning (04-09), midday (10-15), evening (16-21) and       night (22-03), the parameter are therefore a character. It's contructed on hour from the raw dataset. This parameter should help the algorithm,     beacuse it's most likely that an euro-transaction is conducted beteewn 10-15. The reason behind is that in Denmark we use DKK, which means these    transactions are more likely to happen anytime and spontaneously, where euro-transaction is believed to be prepared and with a purpose.

- Weekday
  This parameter contains information about day, which are grouped into 2: weekday and weekend, the parameter are therefore a character. It's         contructed on day from the raw dataset. This parameter should help the algorithm to be more precise combined with the other parameters. There are   no specific reason behind.
  
- Payout
  This parameter contains information about the time you get payed (salary, transfer payments etc.), which typical happen beteewn the 27th and 31th   of each month and in relation to this you would withdraw our money beteewn the 1st and 4th. The parameter are therefore logical and contructed on   day and month from the raw dataset. The probability of a DKK-transaction is therefore believed to be very high in this date period.

- Customer
  This parameter contains information about the person behind the transaction, since it's data form SparNord, we can tell if the person is a          customer or not. The parameter are therefore a logical and contructed on card_type with the text "on-us" from the raw dataset. We argue an          euro-transaction are higher among customers of the bank since there are fees on these transactions for the non-customers.
  
- Region
  This parameter contains information about the region, which are grouped into 4: Region Nordjylland, Region Midtjylland, Region Syddanmark, Region   Sj?lland and Region Hovedstaden. The parameter are therefore a character. It's contructed on the longitude and laditude from the raw dataset and    the specific longitude and laditudes belonging to each region. Since it's SparNord data most of the transactions are believed to be conducted in    Region Nordjylland, where the use of euro is modestly.
  
- ATM ID
  This parameter contains information about each specific ATM there are in the dataset which is 113. The parameter are therefore 
  categorically. It's contructed on atm_id from the raw dataset. We are not sure all of the ATMs have the option to withdraw euro, and therefore we   make this parameter to remove the doubt.
  
- Cardtype
  This parameter contains information about each cardtype used in the ATMs, which are grouped into 9 categories and the parameter are therefore a     character. It's contructed on card_type from the raw dataset. We believe some of the cardtypes are more used to withdraw euro then others.
  
This can be presented as:

$$Euro = Holliday + Airport + Time + Weekday + Payout + Customer + Region + ATM.ID + Cardtype$$


### Control of Features

```{r}
load(file = "generated_data/fsATM.Rdata")
load(file = "generated_data/fbATM.Rdata")
load(file = "generated_data/fdata.Rdata")
```




```{r}
fbATM %>% 
  group_by(Euro) %>% 
  summarize(MeanDistance = mean(Airportdist)) %>% kable
```





```{r}
tab <- table(fbATM$Holliday, fbATM$Euro)
tab %>% kable
paste0("Proportion of non-EURO transactions during vacations ", round(tab[2,1] / sum(tab[,1]),2),"%")
paste0("Proportion of EURO transactions during vacations ", round(tab[2,2] / sum(tab[,2]),2),"%")
summary(tab)

```



```{r fig.height=2, fig.width=10}
table(fbATM$Euro, fbATM$Id) %>% 
  prop.table(margin=2) %>% 
  as.data.frame %>% 
  filter(Var1==T) %>% 
  ggplot(aes(reorder(Var2, Freq), Freq)) + 
  geom_col(fill=base, color=base) + 
  labs(x="ATM id", legend="Euro", title="Percentage of EURO transactions by each ATM machine", 
       subtitle="Machine 99 has almost 50% Euro transactions, but it's an internal ATM") + 
  theme(axis.text.x=element_text(size=4))

```





## Unsupervised Models
First we look whether there is some missing data in the variables:

```{r fig.height=3, fig.width=10}
vars <- c("day","hour","atm_id","atm_street_number","atm_zipcode","atm_lat",
          "atm_lon","message_code","weather_lat","weather_lon","weather_city_id",
          "temp","pressure","humidity","wind_speed","wind_deg","rain_3h",
          "clouds_all","weather_id")

sATM[,vars] %>% 
  aggr(col = c('white',base), 
       labels = names(sATM[,vars]), 
       cex.axis = 0.5, 
       gap = 0.5, 
       ylab=c("Missing data","Pattern"))
```


```{r fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
ggcorr(sATM[vars], label = TRUE, label_size = 2, label_round = 2, hjust = 0.90) + 
  labs(title="Correlation among variables", fill="Correlation") + 
  scale_fill_gradient2(midpoint=0, low=base, mid="white", high=base)+
  coord_cartesian(expand=T)

#unique(sATM$message_code)
#summary(sATM$message_code)
```




### PCA
We do a PCA, but it doesn't explain so much as the data is very hetrogene, with 4 variables the accumuleted variance explained is only 46%

```{r fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
#sATM <- sATM %>% select(-weather_lat, -weather_lon)
vars2 <- c("day","hour","atm_id","atm_street_number","atm_zipcode","atm_lat",
           "atm_lon","weather_city_id","temp","pressure","humidity","wind_speed",
           "wind_deg","clouds_all", "weather_id")

pca <- sATM[,vars2] %>% 
  map_dfc(.f = scale) %>% 
  prcomp(rank=4)

summary(pca)

a <- summary(pca)
df1 <- a$importance[2,] %>% as_tibble %>% gather(variable, value) %>% mutate(type="Change", N=c(1:nrow(.)))
df2 <- a$importance[3,] %>% as_tibble %>% gather(variable, value) %>% mutate(type="Accumulated", N=c(1:nrow(.)))

rbind(df1,df2) %>% as_tibble %>% 
  ggplot(aes(N, value)) + 
  geom_line(size=1.5, color=base) + 
  geom_point(size=3, shape=21, fill="white", color=base, stroke=2) + 
  facet_wrap(~type, scale="free") + scale_x_continuous(breaks=c(0:15)) + 
  labs(title="Screeplot: Number of factors in PCA", x="Number of factors")

loadings <- pca$rotation %>% 
  abs() %>% 
  sweep(2, colSums(.), "/") %>% 
  as.data.frame %>% 
  rownames_to_column("name")

loadings[,-1] <- apply(loadings[,-1], MARGIN = 2, FUN = round, digits=2)

a <- loadings %>% dplyr::select(name, PC1) %>% arrange(desc(PC1)) %>% head(6)
b <- loadings %>% dplyr::select(name, PC2) %>% arrange(desc(PC2)) %>% head(6)
c <- loadings %>% dplyr::select(name, PC3) %>% arrange(desc(PC3)) %>% head(6)
d <- loadings %>% dplyr::select(name, PC4) %>% arrange(desc(PC4)) %>% head(6)

kable(cbind(a,b,c,d)) %>% kable_styling("condensed")

#pca1 <- sATM[,vars2] %>%
#  PCA(scale.unit = TRUE, graph = FALSE)
#pca1 %>% 
#  fviz_screeplot(addlabels = TRUE, 
#                 ncp = 10, 
#                 ggtheme = theme_gray())

#pca2 %>% 
#  fviz_screeplot(addlabels = TRUE, 
#                 ncp = 10, 
#                 ggtheme = theme_gray())
#sATM %>% select(-rain_3h,-weather_lon, -weather_lat, -message_code, -message_text)
  
#sATM[,vars2] %>% 
#  scale() %>%
#  fviz_nbclust(kmeans, method = "wss")  
```



### Kmeans
Kmeans clustering is used to seperate the variables into 4 clusters

```{r fig.height=3, fig.width=10}
#km %>% 
#  fviz_cluster(data = sATM[,vars2],
#               ggtheme = theme_gray()) 

wss <- 0
for (i in 1:15) {
  wss[i] <- kmeans(sATM[,vars2], centers = i, nstart=20)$tot.withinss
}

scree <- tibble(wss, N=1:15)

ggplot(scree, aes(N, wss)) + 
  geom_line(size=1.5, color=base) + 
  geom_point(size=3, shape=21, fill="white", color=base, stroke=2) + 
  scale_x_continuous(breaks=c(1:15)) + 
  labs(title="Screeplot: Number of clusters", x="Number of clusters", y="WSS")
```




```{r}
#hc <- sATM[,vars2] %>%
#  hcut(hc_func = "hclust", 
#       k = 4, 
#       stand = TRUE)

#hc %>%
#  fviz_dend( k=4 ,rect = TRUE, cex = 0.5)
```


# Combination

We plot the 

```{r PCA CLUSTER, fig.height=4, fig.width=10}
set.seed(250920198)

km <- sATM[,vars2] %>% 
  scale() %>% 
  kmeans(centers = 4, nstart = 20) 

df <- pca$x %>% as_tibble %>% select(PC1, PC2)
df$cluster <- km$cluster
eigen <- pca$rotation

points <- (km$centers) %*% (eigen) %>% 
  as_tibble %>% 
  mutate(N=c("PC1", "PC2", "PC3", "PC4")) %>% 
  gather(variable, value,-N) %>% 
  spread(key = N, value = value) %>% 
  mutate(variable2=c(2,1,3,4))

df %>% 
  ggplot(aes(PC1,PC2)) + 
  geom_point(aes(color=as.factor(cluster)), alpha=0.3, show.legend = F) + 
  scale_color_tableau(palette = "Classic Cyclic", type = "regular") + 
  scale_fill_tableau(palette = "Classic Cyclic", type = "regular") + 
  geom_point(data=points, aes(PC2,PC1, fill=as.factor(variable2)), inherit.aes = F, size=5, shape=21, color="black") + 
  labs(title="Kmeans clusters displayed in first two PCA components", 
       subtitle="Centroids are created by multiplying the centers by the eigen vector from PCA", 
       fill = "Clusters")
```





```{r warning=FALSE}
sATM %>%
  bind_cols(cluster = km$cluster) %>%
  select(vars2, cluster) %>%
  select(-atm_street_number, -atm_zipcode, -weather_city_id, -weather_id, -atm_id) %>% 
  group_by(cluster) %>%
  mutate(n = n()) %>%
  summarise_all(funs(mean)) %>%
  mutate_all(funs(round(.,2))) %>% 
  kable %>% 
  kable_styling("condensed")
```










## Supervised Models
We're fitting 6 different models to the data. In this we face som challenges 
- Size of the data: A big challenge is that we have alot of observations, 2.5m, and that is way too much to run on our computers for some models, especially if we want to do any kind of parametertuning. 
- Grouped data: Another challenge is that the data is grouped, this means that some algorrithms, like LDA, have problems seperating the classes.
- Signal to noise ratio: The data has few Eurotransactions, and on top of that there are no initutive good variables to seperate whether people get Euro or DKK from the ATM, as we see it the most promising is wheter the transaction is in a holiday period or not
- Factors: Almost all the features consists of factors. This is a challenge because the models seperate them into really large dummy matrices, and it can create problems if there is not enough variation. The only contenious varible we have is distance to nearest airport.

Facing the size challenge has meant that we decided to do the supervised models in two sections. one with normal sampeling, and one with downsampeling. Our models and the relevant tuning parametres are:

1. Logistic regression
2. Elastic Net
  alpha: decides the penalty enforced on the parametres
  lambda: the weight between Lasso and Ridge regression
3. Classification tree
  CP: is a complexity parametre that decides the degree of pruning of the tree
4. Bagged classification tree
  nbagg: describes how manny trees is created and 
5. Random forrest
  mtry: the number of variables that is included in each split
  ntree: number of branches grown after each split 
6. Suport vector macines/classifier
  kernel: We use a linear kernel, but there are manny choices like polynomial 


```{r}
cv <- trainControl(method = "cv", number = 5)

ml <- fbATM
ml$Euro <- as.factor(ifelse(ml$Euro == "TRUE", "Yes", "No"))

index    <- createDataPartition(ml$Euro, p = 0.03, list = FALSE)
training <- ml[index,] 
test     <- ml[-index,] 
```


### Normal sampeling

```{r MODELS, message=FALSE, warning=FALSE}

a <- Sys.time()
fit_log <- train(Euro ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = "glm", 
                 family    = "binomial",
                 metric    = 'Accuracy')
b <- Sys.time()
fit_ela <- train(Euro ~ .,
                 data      = training,
                 trControl = cv, 
                 tuneGrid  = expand.grid(alpha  = seq(0, 1, by = 0.2), 
                                         lambda = 10^seq(1, -4, by = -1)),
                 method    = "glmnet", 
                 family    = "binomial",
                 metric    = 'Accuracy')

c <- Sys.time()
fit_tre <- train(Euro ~ .,
                 data      = training, 
                 trControl = cv, 
                 metric    = "Kappa",
                 method    = "rpart", 
                 tuneGrid  = expand.grid(cp = c(0.001, 0.005)))
d <- Sys.time()

fit_bag <- train(Euro ~ ., 
                 data      = training, 
                 method    = "treebag",
                 nbagg     = 50,
                 metric    = "ROC",
                 trControl = trainControl(method = "cv", number = 5, classProbs = T))
e <- Sys.time()

fit_raf <- train(Euro ~ ., 
                 data      = sample_n(training, 2000),
                 trControl = cv,
                 .mtry     = 6,
                 ntree     = 100,
                 method    = 'rf',
                 metric    = 'Accuracy')
f <- Sys.time()
fit_svm <- train(Euro ~ ., 
                 data      = sample_n(training, 500),
                 trControl = cv,
                 tuneGrid  = expand.grid(C = 10^seq(1, -1, by = -0.02)),
                 method    = 'svmLinear',
                 metric    = 'Accuracy')
g<- Sys.time()
```


```{r TIME}
cat(paste0("Logistic: ", round(difftime(b,a, units = "mins"),2),"\n",
           "Elastic:  ", round(difftime(c,b, units = "mins"),2),"\n",
           "Tree:     ", round(difftime(d,c, units = "mins"),2),"\n",
           "Bagged:   ", round(difftime(e,d, units = "mins"),2),"\n",
           "RF:       ", round(difftime(f,e, units = "mins"),2),"\n",
           "SVM:      ", round(difftime(g,f, units = "mins"),2),"\n",
           "Total:    ", round(difftime(g,a, units = "mins"),2),"\n"))

```



```{r CONF}
conf_bag <- table(predict(fit_bag,  test), test$Euro)
#conf_bag

conf_tre <- table(predict(fit_tre,  test), test$Euro)
#conf_tre

conf_log <- table(predict(fit_log,  test), test$Euro)
#conf_log

conf_ela <- table(predict(fit_ela,  test), test$Euro)
#conf_log

conf_raf <- table(predict(fit_raf,  test), test$Euro)
#conf_raf

conf_svm <- table(predict(fit_svm,  test), test$Euro)
#conf_svm

random   <- rbinom(n = length(test$Euro), size = 1, prob = mean(training$Euro == "Yes"))
conf_ran <- table(as.factor(if_else(random == 1, "Yes", "No")), test$Euro)
#conf_ran
```



```{r}
kable(cbind(conf_bag, conf_tre, conf_log, conf_ela, conf_raf, conf_svm, conf_ran)) %>% 
  kable_styling("bordered", "condensed") %>%
  column_spec(1, bold = T, color="black") %>%
  add_header_above(c(" "                             = 1, 
                     "Bagged\nTree"                  = 2, 
                     "Classification\nTree"          = 2, 
                     "Logistic\nRegression"          = 2, 
                     "Elastic Net\nRegression"       = 2,
                     "Random\nForrest"               = 2, 
                     "Support Vector\nMachines"      = 2,
                     "Random\nAssignment"            = 2))
```





```{r fig.height=6, fig.width=10}
bag <- rbind(spec(conf_bag), precision(conf_bag), accuracy(conf_bag), recall(conf_bag), npv(conf_bag))
tre <- rbind(spec(conf_tre), precision(conf_tre), accuracy(conf_tre), recall(conf_tre), npv(conf_tre))
raf <- rbind(spec(conf_raf), precision(conf_raf), accuracy(conf_raf), recall(conf_raf), npv(conf_raf))
log <- rbind(spec(conf_log), precision(conf_log), accuracy(conf_log), recall(conf_log), npv(conf_log))
ela <- rbind(spec(conf_ela), precision(conf_ela), accuracy(conf_ela), recall(conf_ela), npv(conf_ela))
ran <- rbind(spec(conf_ran), precision(conf_ran), accuracy(conf_ran), recall(conf_ran), npv(conf_ran))
svm <- rbind(spec(conf_svm), precision(conf_svm), accuracy(conf_svm), recall(conf_svm), npv(conf_svm))

bag$model <- "Bagged Tree"
tre$model <- "Classification Tree"
raf$model <- "Random Forrest"
log$model <- "Logistic Regression"
ela$model <- "Elastic Net Regression"
ran$model <- "Random Assignment"
svm$model <- "Support Vector Machines"

#df <- rbind(log, lok, ran, net, raf, svm)

df <- rbind(ran, log,ela, tre, bag, svm, raf)

ggplot(df, aes(.metric, .estimate, fill = reorder(model, desc(.estimate)))) + 
  geom_col(position="dodge", width = 0.6) + 
  scale_fill_tableau(palette = "Classic Cyclic", type = "regular") + 
  labs(title="Model performance on predicting Legendary status", 
       subtitle="Crossvalidated 1/5 split", fill="Predictive Model")
```



### Down Sampeling
46439 (1.92 %) of the transactions made in SparNordâ€™s ATMâ€™s are transactions of Euro (we have included USD$ and GBPÂ£ as well since they are even tinier proportions of the transactions). The huge disproportion between Euro and DKK transactions creates a bias towards predicting DKK. To solve this problem undersampling is used. This is done by selecting n samples from the dominating class (DKK), where n is equal to the total number of Euro transactions in the training set. This creates a dataset that is approx. 50/50 split between Euro and DKK transactions. 

This however excludes a lot of information from the majority class in the dataset, as the final set is only of â€¦ observations. An alternative approach could be to use oversampling, where the minority class is re-sampled. This however causes problems with overfitting, as the observations of Euro transactions are duplicated. Due to the size of the dataset undersampling is used.






```{r}
#cv <- trainControl(method = "cv", number = 5, sampling = "down")
#
#ml <- fdata
#ml$Euro <- as.factor(ifelse(ml$Euro == "TRUE", "Yes", "No"))
#
#index    <- createDataPartition(ml$Euro, p = 0.10, list = FALSE)
#training <- ml[index,] 
#test     <- ml[-index,] 
#
#a <- Sys.time()
#fit_log <- train(Euro ~ .,
#                 data      = training,
#                 trControl = cv, 
#                 method    = "glm", 
#                 family    = "binomial",
#                 metric    = 'Accuracy')
#b <- Sys.time()
```

















