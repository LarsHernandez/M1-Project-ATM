---
title: "Project M1"
author: "Andreas, Simon, Jess, Lars"
date: "14/9/2019"
output:
  html_document:
    code_folding: hide
    theme: flatly
    toc: yes
    toc_float:
      collapsed: no
---
# SparNord ATM transactions

data link: https://sparnordopenbanking.com/OpenData

colab link: https://colab.research.google.com/drive/1sxLL6-VgMbpN7uDsgd7F1bpdEmyFsRN1

github link: https://github.com/LarsHernandez/M1-Project-ATM


In this project we would work around the following problem statement. To solve the statement we will use the ML-techniques which the course so far have presented for us.

>Predict whether the transaction in the Spar Nord ATMs is a euro transaction or not. 

In order to answer our problem statement, we’ve obtained a data set published by Spar Nord A/S. The data set contains data of all cash withdrawals made at their ATMs in Denmark in 2017. Based on the data set, we then want to set up a model which can 'predict' whether the transaction in the ATMs is in euros or not.

The raw-dataset contains 2.5 million observations with 33 variables, which range from information about day, time and month to weather conditions, coordinates and cardtype. Some of the variables include NA, but those are primarily message codes and text related to the withdrawal (some kind of error). This mean the dataset are pretty complete and there are no need for data imputation or cleaning. The next step is to modify the data to our problem statement, which is descried in feature generation. Before we get to that section, we will present a visually overview of the raw-data with summary statistics and explanatory comments. 

```{r eval=FALSE, include=FALSE}
#pacman::p_load(tidyverse, ggforce, caret, VIM, gridExtra, 
#               ggmap, osmdata, sf, geosphere, yardstick, 
#               knitr, ggthemes, ggridges, kableExtra)
```


```{r PACKAGES, message=FALSE, warning=FALSE}
timer <- Sys.time()

library(tidyverse)
library(lubridate)
library(ggforce)
library(caret)
library(yardstick)
library(knitr)
library(ggthemes)
library(ggridges)
library(kableExtra)
library(gridExtra)
library(VIM)
library(ggridges)
library(ggmap)
library(osmdata)
library(sf)
library(GGally)
library(FactoMineR)
library(geosphere)
library(ROSE)

base  <- "#1f83b4"
base2 <- "#12a2a8"

set.seed(20092019)

# To generate this part requires you have run the datamanagement parts 
# of the script, or you can find the data in the github repository or by
# the codes here below

#load(url("https://github.com/LarsHernandez/M1-Project-ATM/blob/master/generated_data/data.Rdata?raw=true"))
#load(url("https://github.com/LarsHernandez/M1-Project-ATM/blob/master/generated_data/fdata.Rdata?raw=true"))
#load(url("https://github.com/LarsHernandez/M1-Project-ATM/blob/master/generated_data/fbATM.Rdata?raw=true"))
#load(url("https://github.com/LarsHernandez/M1-Project-ATM/blob/master/generated_data/bATM.Rdata?raw=true"))
#load(url("https://github.com/LarsHernandez/M1-Project-ATM/blob/master/generated_data/sATM.Rdata?raw=true"))

load(file = "generated_data/data.Rdata")
load(file = "generated_data/fdata.Rdata")
load(file = "generated_data/fbATM.Rdata")
load(file = "generated_data/bATM.Rdata")
load(file = "generated_data/sATM.Rdata")
```


In the code in this section is the data management. This downloads the main datafile and unzips it. Unzipped the size is around 500mb.

```{r DATA MANAGEMENT, eval=FALSE}
# Data --------------------------------------------------------------------
temp <- tempfile()
download.file("https://sparnordopenbanking.com/downloads/open_dataset.zip", temp)
data1 <- read_csv(unz(temp, "atm_data_part1.csv"))
data2 <- read_csv(unz(temp, "atm_data_part2.csv"), 
                  col_types = cols(rain_3h = col_double()))
unlink(temp)

# Binding the data --------------------------------------------------------
data      <- bind_rows(data1, data2)
data$date <- ymd(paste0(data$year, "-", data$month, "-", data$day))

# Save sample -------------------------------------------------------------

sATM <- sample_n(data, size = 10000,  replace = F)
bATM <- sample_n(data, size = 250000, replace = F)

save(sATM, file = "generated_data/sATM.Rdata")
save(bATM, file = "generated_data/bATM.Rdata")
save(data, file = "generated_data/data.Rdata")

```




## Data
One of the most common errors in data science is not looking at the data, so we will start with this. The dataset includes 113 different ATM's from SparNord placed around Denmark. Each include coordinates, which will be usefull later for feature generation, but we can use these to plot the machines in a map.

```{r MAP, fig.height=6, fig.width=11, message=FALSE, warning=FALSE}
coord <- as.matrix(data.frame(min = c(8, 55), 
                              max = c(13, 58), 
                              row.names = c("x","y")))

s <- data %>% filter(atm_lat > coord[2,1] & atm_lat < coord[2,2] & 
                     atm_lon < coord[1,2] & atm_lon > coord[1,1]) %>% 
  group_by(atm_id) %>% 
  summarize(n   = n(),
            lat = first(atm_lat),
            lon = first(atm_lon),
            man = first(atm_manufacturer))


# First map ---------------------------------------------------------------
map9  <- get_stamenmap(coord, zoom = 9,  maptype = "toner-lite", force = TRUE)

p1 <- ggmap(map9) +
  geom_point(data = s, aes(lon,lat, size=n/1000), alpha = 0.8) + 
  scale_size(range = c(0.5, 8)) +
  labs(title="SparNord ATM transactions in 2017", 
       subtitle = "Size equals number of transactions in thousands", size="N")

p2 <- ggmap(map9) +
  stat_density_2d(data = s, aes(lon,lat, fill = ..level..), geom = "polygon", alpha = .4) +
  #scale_fill_viridis_c(option = "magma") +
  scale_fill_gradient2(midpoint=0.5,low = "black", mid = base, high = "white") +
  labs(title="SparNord ATM transactions in 2017", 
       subtitle = "Density of ATM placement")

grid.arrange(p1,p2, nrow=1)
```

Quick summary of transactions by region

```{r}
fdata %>% group_by(Region) %>% summarise(N=n()) %>% t() %>% kable() %>% kable_styling("condensed", "bordered")
```



In this first visualisation we see that the activity is based heavily around Nordjylland, which is not surprising if you know the bank. We also see that there is a substantial difference in the number of transactions at each machine.

Well, lets see how the transactions are distributed in time:

```{r fig.height=6, fig.width=10}
s1 <- data %>% group_by(month) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(month,n))   + geom_col(fill=base, width=0.8)     + 
  labs(title = "By month - Total", x="") + 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))

s2 <- data %>% group_by(day) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(day,n)) + geom_col(fill=base, color=base)         + 
  labs(title = "By day - Total", x="")

s3 <- data %>% group_by(weekday) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(weekday,n)) + geom_col(fill=base, width=0.8) + 
  labs(title = "By weekday - Total", x="") + 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))

s4 <- data %>% group_by(hour) %>% summarize(n=n()/1000) %>% 
  ggplot(aes(hour,n))    + geom_col(fill=base, color=base)       + 
  labs(title = "By hour - Total", x="") + 
  scale_x_continuous(breaks=c(0,3,6,9,12,15,18,21,24))


s5 <- data %>% filter(currency == "EUR") %>% group_by(weekday) %>% summarize(n=n()/1000) %>% ggplot(aes(weekday,n)) + geom_col(fill=base, width=0.8)  + labs(title="By weekday - Euro", x="")+ 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))
s6 <- data %>% filter(currency == "EUR") %>% group_by(month) %>% summarize(n=n()/1000) %>% ggplot(aes(month,n)) + geom_col(fill=base, width=0.8)  + labs(title="By month - Euro", x="")+ 
  theme(axis.text.x = element_text(angle = 90, vjust=-0.1))
s7 <- data %>% filter(currency == "EUR") %>% group_by(day) %>% summarize(n=n()/1000) %>% ggplot(aes(day,n)) + geom_col(color = base, fill = base) + 
labs(title="By day - Euro", x="")
s8 <- data %>% filter(currency == "EUR") %>% group_by(hour) %>% summarize(n=n()/1000) %>% ggplot(aes(hour,n)) + geom_col(color = base, fill = base) + labs(title="By hour - Euro", x="") + 
  scale_x_continuous(breaks=c(0,3,6,9,12,15,18,21,24))

grid.arrange(s1,s3,s2,s4,s6,s5,s7,s8, nrow=2)
```
The first 4 plots shows the amount of withdraws destributed on weekday, month, day and hour. We can see a pattern in the plots, where there is about 20.000 withdraws each month, except from the summer months 'May','June' and'July' where there is about 25.000 withdraws. This could be explained by the fact that people are more likely to take on vacation in these months. Further it can be seen that people are more likely to make a withdraw in the beginning of the month, where we can see that amount of withdraws is highest on the 1st of the month, and right before the weekend, where friday is the day with the highest amount of withdraws. 

The plots shows the amount of the euro-transactions destributed on weekday, month, day and hour. We can see from the euro-month plot that people are more likely to withdraw euros in the summermonth July. This may be due to more people are going on vacations in the holiday-months like july. And like the case before we can see that people is slightly more likely to withdraw euros in the days leading up to the weekend and in the beginning of the months.

```{r fig.height=5, fig.width=10}

data %>% filter(currency =="EUR") %>% group_by(day, month) %>% summarize(n=n()) %>% ggplot(aes(day, n)) + geom_col(fill=base, color=base) + facet_wrap(~month)  + labs(title="The number of Euro withdrawals on each date", subtitle="We see how the summer is an active period, while january is almost empty")

```

The plots shows the amount of euro-withdraws on the specific day in the month. As can be seen from the plot the amount of euro-withdraws increases from the end of june to the end of july Which can be explained by the fact that people are going on vacation and the currency in that country is euro. It seems like people are more likely to withdraw euro in the holiday period. 


Another thing is to see which machines that the eurotransacrions occour in, and we find that the transactions are not evenly distributed

```{r fig.height=2, fig.width=10}
table(fdata$Euro, fdata$Id) %>% 
  prop.table(margin=2) %>% 
  as.data.frame %>% 
  filter(Var1==T) %>% 
  ggplot(aes(reorder(Var2, Freq), Freq)) + 
  geom_col(fill=base, color=base) + 
  labs(x="ATM id", legend="Euro", title="Percentage of EURO transactions by each ATM machine", 
       subtitle="Machine 99 has almost 50% Euro transactions, but it's an internal ATM with relatively few withdrawals") + 
  theme(axis.text.x=element_text(size=4))
```





## Feature generation

```{r eval=FALSE, message=FALSE, warning=FALSE}
# Features for the data ---------------------------------------------------
#sATM <- bATM

# Holidays ----------------------------------------------------------------
# Ferie i Nordjylland (http://skoleferie-dk.dk/skoleferie-nordjylland/)

Vinter <- ifelse(( sATM$month == "February" & 11 <= sATM$day & sATM$day <= 26), TRUE, FALSE)
Paaske <- ifelse(( sATM$month == "April"    & 8  <= sATM$day & sATM$day <= 17), TRUE, FALSE)
Efter  <- ifelse(( sATM$month == "October"  & 14 <= sATM$day & sATM$day <= 22), TRUE, FALSE)
Jul    <- ifelse(((sATM$month == "December" & 20 <= sATM$day) | (sATM$month=="January" & sATM$day<=4)), TRUE, FALSE)
Sommer <- ifelse(((sATM$month == "June"     & 23 <= sATM$day) | (sATM$month=="July"    & sATM$day>=1)| (sATM$month=="August" & sATM$day<=13)), TRUE, FALSE)
sATM$Holliday <- Jul | Vinter | Paaske | Sommer | Efter

# Distance to other ATMs --------------------------------------------------

mat <- as.matrix(data.frame(lon = sATM$atm_lon, 
                            lat = sATM$atm_lat))

Aal.luft <- c(9.842829962, 57.088999644)
Bil.luft <- c(9.150999396, 55.73749705)
Cph.luft <- c(12.650462,   55.620750)

#Distance is in meter and a straight line:
sATM$Aal.luft <- distHaversine(mat, Aal.luft, r = 6378137)
sATM$Bil.luft <- distHaversine(mat, Bil.luft, r = 6378137)
sATM$Cph.luft <- distHaversine(mat, Cph.luft, r = 6378137)

#Nearest airport in km:
sATM <- sATM %>% 
  mutate(Airportdist = round(pmin(sATM$Aal.luft,sATM$Bil.luft,sATM$Cph.luft)/1000,2))

#Divid clock
sATM$Time <- ifelse((sATM$hour >= 4  & sATM$hour <= 9),  "Morning", NA)
sATM$Time <- ifelse((sATM$hour >= 10 & sATM$hour <= 15), "Midday",  sATM$Time)
sATM$Time <- ifelse((sATM$hour >= 16 & sATM$hour <= 21), "Evening", sATM$Time)
sATM$Time <- ifelse((sATM$hour >= 22 | sATM$hour <= 3),  "Night",   sATM$Time)

#Divid the days
sATM$Weekend <- ifelse(sATM$weekday == "Saturday" | sATM$weekday == "Sunday", TRUE, FALSE)
#Valuta
sATM$Euro <- as.factor(ifelse(sATM$currency=="DKK", FALSE, TRUE))
#Payout
sATM$Payout <- ifelse((sATM$day <=4 | sATM$day >= 27), TRUE, FALSE)
#Customer
sATM$Customer <- str_detect(sATM$card_type, pattern = "on-us")

#Cardtype
sATM$Card <- sapply(strsplit(sATM$card_type, split = ' - on-us', fixed = TRUE), function(x) (x[1]))
sATM$Card <- str_to_lower(sATM$Card)
sATM$Card <- Hmisc::capitalize(sATM$Card)
sATM$Card <- as.factor(sATM$Card)

#Region
loc <- sATM %>% group_by(atm_id) %>% 
  summarize(x=first(atm_lon), y=first(atm_lat))

reg <- NA
for (i in 1:nrow(loc)) {
  aa <- read_csv(paste0("http://dawa.aws.dk/kommuner/reverse?x=",loc$x[i] ,"&y=",loc$y[i]))
  reg[i] <- as.character(aa[23,])
}

loc$Region <- reg

sATM <- merge(sATM,loc[,c(1,4)],by ="atm_id")
sATM$Region <- sapply(strsplit(sATM$Region, split='": "', fixed=TRUE), function(x) (x[2]))
sATM$Region <- sapply(strsplit(sATM$Region, split='"', fixed=TRUE), function(x) (x[1]))
sATM$Region <- as.factor(sATM$Region)

sATM$Id       <- as.factor(sATM$atm_id)
sATM$Time     <- as.factor(sATM$Time)
sATM$Holliday <- as.factor(sATM$Holliday)
sATM$Customer <- as.factor(sATM$Customer)
sATM$Weekend  <- as.factor(sATM$Weekend)
sATM$Payout   <- as.factor(sATM$Payout)

#Final dataset
fsATM <- sATM %>% select(Id, Holliday, Airportdist, Time, 
                         Euro, Customer, Card, Weekend, Payout, Region)

#save(fsATM, file = "generated_data/fsATM.Rdata")
```



The first step is to prepare the raw data for our analysis. The raw dataset doesn't need to be cleaned, because it's pretty complete for our purpose. Based on the problem statement we want the algorithm to predict Euro or not Euro transaction from the following parameters:

- **Holliday:**
  This parameter contains information about the specific day, is it a "normal" day or a day in the vacation, the parameter are therefore logical.     It's contructed on day and month from the raw dataset. Information about vacation are taken from: http://skoleferie-dk.dk/skoleferie-nordjylland/.   We argue that the purpose for Euro are bigger in the vacation then else, which should provide usefull information to the algorithm.
  
- **Airport distance:**
  This parameter contains information about the distance to the closest airport, the parameter are therefore numerical. It's contructed on the        longitude and laditude on each ATM from the raw dataset and the included airports. The included airports are Aalborg, Billund and Copenhagen. The   hypotese is that, when flying your destination are likely an Euro-country, which means that ATM closest to an airport would be used to get euro.
  
- **Time:**
  This parameter contains information about the time of the day, which are grouped into 4: morning (04-09), midday (10-15), evening (16-21) and       night (22-03), the parameter are therefore a character. It's contructed on hour from the raw dataset. This parameter should help the algorithm,     beacuse it's most likely that an euro-transaction is conducted beteewn 10-15. The reason behind is that in Denmark we use DKK, which means these    transactions are more likely to happen anytime and spontaneously, where euro-transaction is believed to be prepared and with a purpose.

- **Weekday:**
  This parameter contains information about day, which are grouped into 2: weekday and weekend, the parameter are therefore a character. It's         contructed on day from the raw dataset. This parameter should help the algorithm to be more precise combined with the other parameters. There are   no specific reason behind.
  
- **Payout:**
  This parameter contains information about the time you get payed (salary, transfer payments etc.), which typical happen beteewn the 27th and 31th   of each month and in relation to this you would withdraw our money beteewn the 1st and 4th. The parameter are therefore logical and contructed on   day and month from the raw dataset. The probability of a DKK-transaction is therefore believed to be very high in this date period.

- **Customer:**
  This parameter contains information about the person behind the transaction, since it's data form SparNord, we can tell if the person is a          customer or not. The parameter are therefore a logical and contructed on card_type with the text "on-us" from the raw dataset. We argue an          euro-transaction are higher among customers of the bank since there are fees on these transactions for the non-customers.
  
- **Region:**
  This parameter contains information about the region, which are grouped into 4: Region Nordjylland, Region Midtjylland, Region Syddanmark, Region   Sj?lland and Region Hovedstaden. The parameter are therefore a character. It's contructed on the longitude and laditude from the raw dataset and    the specific longitude and laditudes belonging to each region. Since it's SparNord data most of the transactions are believed to be conducted in    Region Nordjylland, where the use of euro is modestly.
  
- **ATM ID:**
  This parameter contains information about each specific ATM there are in the dataset which is 113. The parameter are therefore 
  categorically. It's contructed on atm_id from the raw dataset. We are not sure all of the ATMs have the option to withdraw euro, and therefore we   make this parameter to remove the doubt.
  
- **Cardtype:**
  This parameter contains information about each cardtype used in the ATMs, which are grouped into 9 categories and the parameter are therefore a     character. It's contructed on card_type from the raw dataset. We believe some of the cardtypes are more used to withdraw euro then others.
  
This can be presented as:

$$Euro = Holliday + Airport + Time + Weekday + Payout + Customer + Region + ATM.ID + Cardtype$$

### Plots of features

These plots illustrates the relationship between euro and the distance to the nearest airport. The first one shows a Histogram of the amount of withdraws per  ATM w. distance the ATMs nearest airport, where the second shows the same but only with euro-withdraws. You could emagine that people who are going on vacation would use the ATMs close to the airport when they need euros. As you can see from the last plot, which plots the density of Airportdistance and euro, nearly 20% of the euro-transactions is made in the ATM which is closest to the airport, and after that there seems to be a downward trend in the density. 

```{r fig.height=5, fig.width=10}

a3 <- ggplot(fdata, aes(x=fdata$Airportdist)) + 
  geom_histogram(binwidth=3, col = base, fill = base) + 
  labs(title="Distance to airport total", x="Distance to airport")

a4 <- ggplot(fdata, aes(x=fdata$Airportdist, stat(density))) + 
  facet_wrap(~Euro, nrow=2) + 
  geom_area(aes(y = ..density..), stat = 'density', fill=base) + 
  labs(title="Distance to airport by Euro", x="Distance to airport")

grid.arrange(a3,a4, nrow=1)
```



If we do a summary of average distance to airport by currency we find that contrary to expectation the average distance is higher for Eurotransactions

```{r}
fdata %>% 
  group_by(Euro) %>% 
  summarize(MeanDistance = mean(Airportdist)) %>% kable %>% kable_styling("condensed", "bordered")
```

It seems like Region Sjælland and Region Midtjylland are more likely to withdraw euro than the rest of Denmark. 3.27% of the transactions in Region Sjælland are euro-withdraw and 2.99% in Region Midtjylland. The numbers for Region Syddanmark, Hovedstaden and Nordjylland is 2.13%, 1.55% and 1.43%. This could for example be explained by the fact that there are some bigger airports in Region Midtjylland and Region Sjælland, with more flights and some other destination than for example Aalborg airport.

```{r fig.height=4, fig.width=10}
#Euro/Region
fdata %>% group_by(Region, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(reorder(Region,freq),freq)) + geom_col(fill=base) + labs(title="Euro withdraws by region", x="") + geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))
```

- As we can see there is total 16.965 euro-withdraws in the Holiday, and 29.474 not-euro-withdraws and therefore an overweight of not-euro-withdraws in the holiday. The plot shows that 2.49% of the withdraws in the holiday is euro-withdraws, where 1.65% of the withdraws outside of the holiday is euro-withdraws. So it seems like people are more likely to do euro-withdraws in the holiday than outside the holiday. This can be explained by the fact that people want to have some euros in cash if they are going on a vacation in Europe. We can test if this different is statistically significant by a chisq test and we find that it is very significant, therefore this will be a good variable for our model. 

```{r}
tab <- table(fdata$Holliday, fdata$Euro)
summary(tab)
```

- The plot shows that 2.98% of the euro-withdraws are made by customers of the bank, while 0.4% are made by non-customers. 

- The plot shows that 1.27% of the euro-withdraws are made in the 'Weekend', while 2.06% are made outside the weekend. It seems like people are more likely to made euro-withdraws in the weekdays than in the weekend.

- The plot shows that 1.65% of the euro-withdraws are in the payout-period, while 1.99% of the euro-withdraws are in the not-payout-period. This indicates that there is no trend between the payout-period and the euro-withdraws.

```{r fig.height=4, fig.width=10}
a2 <- fdata %>% group_by(Holliday, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(reorder(Holliday,freq),freq)) + geom_col(fill=base) + labs(title="Holiday", x="") + geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))

#Customer/Euro

a5 <- fdata %>% group_by(Customer, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(Customer,freq)) + geom_col(fill=base) + labs(title="Customer", x="") + geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))
#Weekend/Euro

a6 <- fdata %>% group_by(Weekend, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(Weekend,freq)) + geom_col(fill=base) + labs(title="Weekend", x="") + geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))


#Payout/Euro
a7 <- fdata %>% group_by(Payout, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(Payout,freq)) + geom_col(fill=base) + labs(title="Payout", x="") + geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))

grid.arrange(a2, a5,a6,a7, nrow=1)
```



The plot shows which cardtype that often has a euro-withdraw, as can be seen it's the 'Visa Dankort' which has the highest frequency of euro-withdraws. The plot shows that 3.15% of the withdraws in 'Visa Dankort' is euro-withdraws.

```{r fig.height=4, fig.width=10}
fdata %>% group_by(Card, Euro) %>% summarize(n=n()) %>% mutate(freq = n/sum(n)) %>% filter(Euro==T) %>% 
  ggplot(aes(reorder(Card,freq),freq)) + 
  geom_col(fill=base) + 
  labs(title="Euro withdraws by cardtype", x="") + 
  geom_text(aes(y=freq + 0.002,label=round(freq*100,2)))

```

Based on this review and plots of the new variables, we have the dataset for the supervised ML-part of the project. To summarize, the set includes 2.5 million observations across 10 variables, which is primarily factors. It’s only the variable Airport which is numerical, therefore we don’t find it relevant to scale this data.





## Unsupervised Models
This section we'll focus on the unsupervised ML part of the project. The dataset is also used on the raw-dataset, but it’s only the numerical variables which is included in the model. First we check the data for missing values, see the figure below. Here it’s clear that the variables “message_text” and “rain_3h” doesn’t contains many values, which is why we exclude these variables.

```{r fig.height=3, fig.width=10}
vars <- c("day","hour","atm_id","atm_street_number","atm_zipcode","atm_lat",
          "atm_lon","message_code","weather_lat","weather_lon","weather_city_id",
          "temp","pressure","humidity","wind_speed","wind_deg","rain_3h",
          "clouds_all","weather_id")

bATM[,vars] %>% 
  aggr(col = c('white',base), 
       labels = names(bATM[,vars]), 
       cex.axis = 0.5, 
       gap = 0.5, 
       ylab=c("Missing data","Pattern"))
```

The next step is to evaluate the correlation between the remaining variables. The case is if the correlation between two variables is high we'll exclude one of the variables. The reason is that high correlation doesn’t contribute with useful information, since it’s already there in one of the two variables. In the plot below the correlation between the variables is shown.

```{r fig.height=5, fig.width=10, message=FALSE, warning=FALSE}
ggcorr(bATM[vars], label = TRUE, label_size = 2, label_round = 2, hjust = 0.90) + 
  labs(title="Correlation among variables", fill="Correlation") + 
  scale_fill_gradient2(midpoint=0, low=base, mid="white", high=base)+
  coord_cartesian(expand=T)
```

As it can be seen the correlation is high between “weather_lon” and “atm_lon” plus “weather_lat” and “atm_lat and because of this we decide to exclude the two weather variables. That means the dataset for the unsupervised ML consist of 2.5 million observations and 15 variables. Finally we scale the dataset to normalize the values.

### PCA
We are now ready to make a PCA of the dataset. We do this because we want to reduce the dimensionality of the dataset. The PCA works by projecting our data into a lower dimensional subspace which hopefully extracts the essence of the variation in the data.

```{r fig.height=3, fig.width=10, message=FALSE, warning=FALSE}
vars2 <- c("day","hour","atm_id","atm_street_number","atm_zipcode","atm_lat",
           "atm_lon","weather_city_id","temp","pressure","humidity","wind_speed",
           "wind_deg","clouds_all", "weather_id")
pca <- sATM[,vars2] %>% 
  map_dfc(.f = scale) %>% 
  prcomp(rank=4)

a <- summary(pca)

df1 <- a$importance[2,] %>% as_tibble %>% gather(variable, value) %>% mutate(type="Change", N=c(1:nrow(.)))
df2 <- a$importance[3,] %>% as_tibble %>% gather(variable, value) %>% mutate(type="Accumulated", N=c(1:nrow(.)))

rbind(df1,df2) %>% as_tibble %>% 
  ggplot(aes(N, value)) + 
  geom_line(size=1.5, color=base) + 
  geom_point(size=3, shape=21, fill="white", color=base, stroke=2) + 
  facet_wrap(~type, scale="free") + scale_x_continuous(breaks=c(0:15)) + 
  labs(title="Screeplot: Number of factors in PCA", x="Number of factors")
```

If we use the elbow-method we can see on the plot above that the optimal number of PCA’s seems to be between 3 to 5 PCAs. If we then choose to go with 4 PCAs then the accumulated variance explained is only 46%, that means that the data is very heterogenic. If we examines the selected PCAs we can see that the top loading in each component is respectively “atm_zipcode”, “clouds_all”, “atm_id” and “wind_speed”.

```{r fig.height=3, fig.width=10, message=FALSE, warning=FALSE}

loadings <- pca$rotation %>% 
  abs() %>% 
  sweep(2, colSums(.), "/") %>% 
  as.data.frame %>% 
  rownames_to_column("name")

loadings[,-1] <- apply(loadings[,-1], MARGIN = 2, FUN = round, digits=2)

a <- loadings %>% dplyr::select(name, PC1) %>% arrange(desc(PC1)) %>% head(6)
b <- loadings %>% dplyr::select(name, PC2) %>% arrange(desc(PC2)) %>% head(6)
c <- loadings %>% dplyr::select(name, PC3) %>% arrange(desc(PC3)) %>% head(6)
d <- loadings %>% dplyr::select(name, PC4) %>% arrange(desc(PC4)) %>% head(6)

kable(cbind(a,b,c,d)) %>% kable_styling("condensed", "bordered")
```

### Kmeans
After the PCA we create a Kmeans. There are no relations between the two methods, but they can be used within the decision to select n PCA’s or k clusters. Kmeans is dividing the dataset into k clusters, where the basic idea is to minimize the total within-cluster variation. This is a method to discover similarities in the dataset, and then sample the similar groups into clusters, and the ideal situation is that there is great seperation between the clusters. 

|               | Supervised    | Unsupervised       |
| ------------- |:-------------:| ------------------:|
| Discrete      | Classification| Clustering (Kmeans)|
| Continuous    | Regression    | Dim.reduction (PCA)|



```{r SCREE, fig.height=3, fig.width=10}
wss <- 0
for (i in 1:15) {
  wss[i] <- kmeans(sATM[,vars2], centers = i, nstart = 5)$tot.withinss
}

scree <- tibble(wss, N = 1:15)

ggplot(scree, aes(N, wss)) + 
  geom_line(size = 1.5, color = base) + 
  geom_point(size = 3, shape = 21, fill="white", color = base, stroke = 2) + 
  scale_x_continuous(breaks = c(1:15)) + 
  labs(title="Screeplot: Number of clusters", x = "Number of clusters", y = "WSS")
```

By use of the elbow-method we conclude that the optimal number of clusters is between 2 to 4 clusters.

### Combination
We select 4 clusters and then plot these in two of the dimensions from the PCA. This plot shows that there are no clear separation between the 4 clusters. The table is supporting that there are no clear difference to be found in the dataset.

```{r PCA CLUSTER, fig.height=4, fig.width=10}
set.seed(250920198)

km <- sATM[,vars2] %>% 
  scale() %>% 
  kmeans(centers = 4, nstart = 20) 

df <- pca$x %>% as_tibble %>% select(PC1, PC2)
df$cluster <- km$cluster
eigen <- pca$rotation

points <- (km$centers) %*% (eigen) %>% 
  as_tibble %>% 
  mutate(N=c("PC1", "PC2", "PC3", "PC4")) %>% 
  gather(variable, value,-N) %>% 
  spread(key = N, value = value) %>% 
  mutate(variable2=c(2,1,3,4))

df %>% 
  ggplot(aes(PC1,PC2)) + 
  geom_point(aes(color=as.factor(cluster)), alpha=0.3, show.legend = F) + 
  scale_color_tableau(palette = "Classic Cyclic", type = "regular") + 
  scale_fill_tableau(palette = "Classic Cyclic", type = "regular") + 
  geom_point(data = points, aes(PC2,PC1, fill = as.factor(variable2)), 
             inherit.aes = F, size = 5, shape = 21, color = "black") + 
  labs(title = "Kmeans clusters displayed in first two PCA components", 
       subtitle = "Centroids are created by multiplying the centers by the eigen vector from PCA", 
       fill = "Clusters")
```

```{r warning=FALSE}
sATM %>%
  bind_cols(cluster = km$cluster) %>%
  select(vars2, cluster) %>%
  select(-atm_street_number, -atm_zipcode, -weather_city_id, -weather_id, -atm_id) %>% 
  group_by(cluster) %>%
  mutate(n = n()) %>%
  summarise_all(funs(mean)) %>%
  mutate_all(funs(round(.,2))) %>% 
  kable %>% 
  kable_styling("condensed", "bordered")
```



## Supervised Models
We're fitting 6 different models to the data. In this we face som challenges 

- Size of the data: A big challenge is that we have alot of observations, 2.5m, and that is way too much to run on our computers for some models, especially if we want to do any kind of parametertuning. 
- Grouped data: Another challenge is that the data is grouped, this means that some algorrithms, like LDA, have problems seperating the classes.
- Signal to noise ratio: The data has few Eurotransactions, and on top of that there are no initutive good variables to seperate whether people get Euro or DKK from the ATM, as we see it the most promising is wheter the transaction is in a holiday period or not
- Factors: Almost all the features consists of factors. This is a challenge because the models seperate them into really large dummy matrices, and it can create problems if there is not enough variation. The only contenious varible we have is distance to nearest airport.

Facing the size challenge has meant that we decided to do the supervised models in two sections. one with normal sampeling, and one with downsampeling. Our models and the relevant tuning parametres are:

1. Logistic regression
2. Elastic Net

   *alpha:* decides the penalty enforced on the parametres
   
   *lambda:* the weight between Lasso and Ridge regression

3. Classification tree

   *CP:* is a complexity parametre that decides the degree of pruning of the tree
   
4. Bagged classification tree

   *nbagg:* describes how manny trees is created and 
   
5. Random forrest

   *mtry:* the number of variables that is included in each split
   
   *ntree:* number of branches grown after each split 
   
6. Suport vector macines/classifier

   *kernel:* We use a linear kernel, but there are manny choices like polynomial 

The crossvalidation is set 5-fold, and the data is split into test and training

```{r}
cv <- trainControl(method = "cv", number = 5)

ml <- fbATM
ml$Euro <- as.factor(ifelse(ml$Euro == "TRUE", "Yes", "No"))

index    <- createDataPartition(ml$Euro, p = 0.04, list = FALSE)
training <- ml[index,] 
test     <- ml[-index,] 
```


### Normal sampeling
The 6 models are fit with `caret`

```{r MODELS, message=FALSE, warning=FALSE}

a <- Sys.time()
fit_log <- train(Euro ~ .,
                 data      = training,
                 trControl = cv, 
                 method    = "glm", 
                 family    = "binomial",
                 metric    = 'Accuracy')
b <- Sys.time()
fit_ela <- train(Euro ~ .,
                 data      = training,
                 trControl = cv, 
                 tuneGrid  = expand.grid(alpha  = seq(0, 1, by = 0.2), 
                                         lambda = 10^seq(1, -4, by = -1)),
                 method    = "glmnet", 
                 family    = "binomial",
                 metric    = 'Accuracy')

c <- Sys.time()
fit_tre <- train(Euro ~ .,
                 data      = training, 
                 trControl = cv, 
                 metric    = "Kappa",
                 method    = "rpart", 
                 tuneGrid  = expand.grid(cp = c(0.001, 0.005)))
d <- Sys.time()

fit_bag <- train(Euro ~ ., 
                 data      = training, 
                 method    = "treebag",
                 nbagg     = 50,
                 metric    = "ROC",
                 trControl = trainControl(method = "cv", number = 5, classProbs = T))
e <- Sys.time()

fit_raf <- train(Euro ~ ., 
                 data      = sample_n(training, 2000),
                 trControl = cv,
                 .mtry     = 6,
                 ntree     = 100,
                 method    = 'rf',
                 metric    = 'Accuracy')
f <- Sys.time()
fit_svm <- train(Euro ~ ., 
                 data      = sample_n(training, 500),
                 trControl = cv,
                 tuneGrid  = expand.grid(C = 10^seq(1, -1, by = -0.02)),
                 method    = 'svmLinear',
                 metric    = 'Accuracy')
g<- Sys.time()
```


Lets look at how long this took to run:

```{r TIME}

cat(paste0("Size of training set: ",nrow(training)," rows of 2.5m\n","\n",
           "Logistic: ", round(difftime(b,a, units = "mins"),2),"\n",
           "Elastic:  ", round(difftime(c,b, units = "mins"),2),"\n",
           "Tree:     ", round(difftime(d,c, units = "mins"),2),"\n",
           "Bagged:   ", round(difftime(e,d, units = "mins"),2),"\n",
           "RF:       ", round(difftime(f,e, units = "mins"),2),"\n",
           "SVM:      ", round(difftime(g,f, units = "mins"),2),"\n",
           "Total:    ", round(difftime(g,a, units = "mins"),2),"\n"))

```
We can see that the tree and logistic regression are really fast, so there is plenty of room to increase the training set for these, from the code it can be seen that SVM only are run with 500 and RF with 2000. We're experimenting with higher for all and will present results at the exam as training the models on 0.1% - 0.5% of the data isn't optimal.


```{r PREDICT, message=FALSE, warning=FALSE}
a <- Sys.time()
conf_bag <- table(predict(fit_bag,  test), test$Euro)
conf_tre <- table(predict(fit_tre,  test), test$Euro)
conf_log <- table(predict(fit_log,  test), test$Euro)
conf_ela <- table(predict(fit_ela,  test), test$Euro)
conf_raf <- table(predict(fit_raf,  test), test$Euro)
conf_svm <- table(predict(fit_svm,  test), test$Euro)
random   <- rbinom(n = length(test$Euro), size = 1, prob = mean(training$Euro == "Yes"))
conf_ran <- table(as.factor(if_else(random == 1, "Yes", "No")), test$Euro)

paste("The prediction takes: ", round(difftime(Sys.time(), a, units = "mins"),2), " mins")
```

The prediction takes some time, especially the bagged tree as the observations has to be run through all 50 trees

```{r}
kable(cbind(conf_bag, conf_tre, conf_log, conf_ela, conf_raf, conf_svm, conf_ran)) %>% 
  kable_styling("bordered", "condensed", font_size = 11) %>%
  column_spec(1, bold = T, color="black") %>%
  add_header_above(c(" "                             = 1, 
                     "Bagged\nTree"                  = 2, 
                     "Classification\nTree"          = 2, 
                     "Logistic\nRegression"          = 2, 
                     "Elastic Net\nRegression"       = 2,
                     "Random\nForrest"               = 2, 
                     "Support Vector\nMachines"      = 2,
                     "Random\nAssignment"            = 2))
```





```{r fig.height=5, fig.width=10}
bag <- rbind(spec(conf_bag), precision(conf_bag), accuracy(conf_bag), recall(conf_bag), npv(conf_bag))
tre <- rbind(spec(conf_tre), precision(conf_tre), accuracy(conf_tre), recall(conf_tre), npv(conf_tre))
raf <- rbind(spec(conf_raf), precision(conf_raf), accuracy(conf_raf), recall(conf_raf), npv(conf_raf))
log <- rbind(spec(conf_log), precision(conf_log), accuracy(conf_log), recall(conf_log), npv(conf_log))
ela <- rbind(spec(conf_ela), precision(conf_ela), accuracy(conf_ela), recall(conf_ela), npv(conf_ela))
ran <- rbind(spec(conf_ran), precision(conf_ran), accuracy(conf_ran), recall(conf_ran), npv(conf_ran))
svm <- rbind(spec(conf_svm), precision(conf_svm), accuracy(conf_svm), recall(conf_svm), npv(conf_svm))

bag$model <- "Bagged Tree"
tre$model <- "Classification Tree"
raf$model <- "Random Forrest"
log$model <- "Logistic Regression"
ela$model <- "Elastic Net Regression"
ran$model <- "Random Assignment"
svm$model <- "Support Vector Machines"

df <- rbind(ran, log,ela, tre, bag, svm, raf)

ggplot(df, aes(.metric, .estimate, fill = reorder(model, desc(.estimate)))) + 
  geom_col(position="dodge", width = 0.6) + 
  scale_fill_tableau(palette = "Classic Cyclic", type = "regular") + 
  labs(title="Model performance on predicting Euro transaction", 
       subtitle="Crossvalidated 1/5 split - Normal sampeling", 
       fill="Predictive Models\nordered by performance")
```

The models archives a high level of accuracy, this however is primarily caused because most of the models only predicts DKK. The spec values are low for all models. The only models that predict Euro withdrawals are the logistic model and the bagged tree. The bagged tree predicts the most Euro withdrawals, but its npv score is lower than for logistic regression. This is due to bagged tree making more predictions for Euro withdrawals compared to the logistic models. Thus the logistic models makes fewer but more “qualified” predictions.

A model that assigns randomly is also included. It randomly predicts Euro withdrawals corresponding to the proportion of Euro in the dataset 

$$spec = \frac{TT}{TF+TT}$$

$$npv = \frac{TT}{FT+TT}$$




### Down Sampeling
46439 (1.92 %) of the transactions made in SparNord’s ATM’s are transactions of Euro (we have included USD$ and GBP£ as well since they are even tinier proportions of the transactions). The huge disproportion between Euro and DKK transactions creates a bias towards predicting DKK. To solve this problem undersampling is used. This is done by selecting n samples from the dominating class (DKK), where n is equal to the total number of Euro transactions in the training set. This creates a dataset that is approx. 50/50 split between Euro and DKK transactions. 

This however excludes a lot of information from the majority class in the dataset, as the final set is only of … observations. An alternative approach could be to use oversampling, where the minority class is re-sampled. This however causes problems with overfitting, as the observations of Euro transactions are duplicated. Due to the size of the dataset undersampling is used.

```{r MODEL DOWN, eval=TRUE, message=FALSE, warning=FALSE}
# simple undersampling  ---------------------------------------------------
#Downsampling is done by adding sampling="down" in trainControl 
#This time the entire dataset with features is used (fdata)

ml <- fdata
ml$Euro  <- as.factor(ifelse(ml$Euro == "TRUE", "Yes", "No"))
index    <- createDataPartition(ml$Euro, p = 0.75, list = FALSE)
training <- ml[index,] 
test     <- ml[-index,] 

sub  <- ovun.sample(Euro ~ ., data = training, p = 0.5, method = "under", seed = 1)$data
sub2 <- sample_n(sub, 20000)
sub3 <- sample_n(sub, 5000)
sub4 <- sample_n(sub, 100)

a <- Sys.time()
fit_log <- train(Euro ~ .,
                 data      = sub3,
                 trControl = cv, 
                 method    = "glm", 
                 family    = "binomial",
                 metric    = 'Accuracy')
b <- Sys.time()
fit_ela <- train(Euro ~ .,
                 data      = sub3,
                 trControl = cv, 
                 tuneGrid  = expand.grid(alpha  = seq(0, 1, by = 0.2), 
                                         lambda = 10^seq(1, -4, by = -1)),
                 method    = "glmnet", 
                 family    = "binomial",
                 metric    = 'Accuracy')
c <- Sys.time()
fit_tre <- train(Euro ~ .,
                 data      = sub3, 
                 trControl = cv, 
                 metric    = "Kappa",
                 method    = "rpart", 
                 tuneGrid  = expand.grid(cp = c(0.001, 0.005)))
d <- Sys.time()
fit_bag <- train(Euro ~ ., 
                 data      = sub3, 
                 method    = "treebag",
                 nbagg     = 20,
                 metric    = "ROC",
                 trControl = trainControl(method = "cv", number = 5, classProbs = T))
e <- Sys.time()
fit_raf <- train(Euro ~ ., 
                 data      = sub3,
                 trControl = cv,
                 .mtry     = 6,
                 ntree     = 50,
                 method    = 'rf',
                 metric    = 'Accuracy')
f <- Sys.time()
fit_svm <- train(Euro ~ ., 
                 data      = sub4,
                 trControl = cv,
                 tuneGrid  = expand.grid(C = 10^seq(1, -1, by = -0.02)),
                 method    = 'svmLinear',
                 metric    = 'Accuracy')
g <- Sys.time()

```


```{r PREDICT DOWN, eval=TRUE}
cat(paste0("Size of training set: ",nrow(training)," rows of 2.5m\n","\n",
           "Logistic: ", round(difftime(b,a, units = "mins"),2),"\n",
           "Elastic:  ", round(difftime(c,b, units = "mins"),2),"\n",
           "Tree:     ", round(difftime(d,c, units = "mins"),2),"\n",
           "Bagged:   ", round(difftime(e,d, units = "mins"),2),"\n",
           "RF:       ", round(difftime(f,e, units = "mins"),2),"\n",
           "SVM:      ", round(difftime(g,f, units = "mins"),2),"\n",
           "Total:    ", round(difftime(g,a, units = "mins"),2),"\n"))

aa <- Sys.time()
conf_log <- table(predict(fit_log,  test), test$Euro)
conf_tre <- table(predict(fit_tre,  test), test$Euro)
conf_ela <- table(predict(fit_ela,  test), test$Euro)
conf_bag <- table(predict(fit_bag,  test), test$Euro)
conf_raf <- table(predict(fit_raf,  test), test$Euro)
conf_svm <- table(predict(fit_svm,  test), test$Euro)
random   <- rbinom(n = length(test$Euro), size = 1, prob = mean(training$Euro == "Yes"))
conf_ran <- table(as.factor(if_else(random == 1, "Yes", "No")), test$Euro)

paste("The prediction takes: ", round(difftime(Sys.time(), aa, units = "mins"),2), " mins")

kable(cbind(conf_bag, conf_tre, conf_log, conf_ela, conf_raf, conf_svm, conf_ran)) %>% 
  kable_styling("bordered", "condensed", font_size = 11) %>%
  column_spec(1, bold = T, color = "black") %>%
  add_header_above(c(" "                             = 1, 
                     "Bagged\nTree"                  = 2, 
                     "Classification\nTree"          = 2, 
                     "Logistic\nRegression"          = 2, 
                     "Elastic Net\nRegression"       = 2,
                     "Random\nForrest"               = 2, 
                     "Support Vector\nMachines"      = 2,
                     "Random\nAssignment"            = 2))
```


```{r eval=TRUE, fig.height=5, fig.width=10}
#confusionMatrix(table(conf_dt3, x_test3$Euro))

raf <- rbind(spec(conf_raf), precision(conf_raf), accuracy(conf_raf), recall(conf_raf), npv(conf_raf))
ela <- rbind(spec(conf_ela), precision(conf_ela), accuracy(conf_ela), recall(conf_ela), npv(conf_ela))
tre <- rbind(spec(conf_tre), precision(conf_tre), accuracy(conf_tre), recall(conf_tre), npv(conf_tre))
log <- rbind(spec(conf_log), precision(conf_log), accuracy(conf_log), recall(conf_log), npv(conf_log))
bag <- rbind(spec(conf_bag), precision(conf_bag), accuracy(conf_bag), recall(conf_bag), npv(conf_bag))
ran <- rbind(spec(conf_ran), precision(conf_ran), accuracy(conf_ran), recall(conf_ran), npv(conf_ran))
svm <- rbind(spec(conf_svm), precision(conf_svm), accuracy(conf_svm), recall(conf_svm), npv(conf_svm))

bag$model <- "Bagged Tree"
tre$model <- "Classification Tree"
raf$model <- "Random Forrest"
log$model <- "Logistic Regression"
ela$model <- "Elastic Net Regression"
ran$model <- "Random Assignment"
svm$model <- "Support Vector Machines"

df <- rbind( log, tre, bag, svm, raf, ran, ela)

ggplot(df, aes(.metric, .estimate, fill = reorder(model, desc(.estimate)))) + 
  geom_col(position = "dodge", width = 0.6) + 
  scale_fill_tableau(palette = "Classic Cyclic", type = "regular") +  
  labs(title = "Model performance on predicting Euro transaction", 
       subtitle = "Crossvalidated 1/5 split - Under/down sampeling", 
       fill = "Predictive Models\nordered by performance")

```



## Conclusion
Using the original training set yielded a high accuracy, this however was primarily due to almost all of the predictions being DKK. The spec values were low for all models. This is not optimal if the purpose is to predict Euro withdrawals. Using downscaling resulted in a decrease in accuracy, however a major part of the Euro withdrawals in the test set was predicted. This resulted in high spec values, but the NPV values were low due to a lot of predicted Euro transactions. 

The models have not performed well and the conclusion must be that the model contains too much noise and/or that the included variables lacks sufficient explanatory power.

The logistic regression seemed to perform the best compared to the other more “advanced” models used for prediction, however these results were still not satisfactory.




```{r}
paste("Total time to knit the document: ", round(difftime(Sys.time(), timer, units = "mins"),2), " mins")
```









